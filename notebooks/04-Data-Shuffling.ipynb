{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Shuffle\n",
    "\n",
    "- Its potentially one of the most performant consuming operation in data distributed systems:\n",
    "    - Network I/O is slow\n",
    "    - Memory pressure\n",
    "    - Disk spills\n",
    "\n",
    "- Shuffling:\n",
    "    - Redistributing data across the cluster\n",
    "    - Occurs when data needs to move between executors, i.e. when joining two data sets\n",
    "    - `groupByKey` `reduceByKey` `Joins`\n",
    "    - Data Skew, uneven distribution of data\n",
    "    - Partitioning\n",
    "    - Caching\n",
    "    - Data locality\n",
    "\n",
    "- `Narrow transformation`: are operations where each partition's data can be processed independently (map, filter, flatmap)\n",
    "- `Wide transformations`: are operations that require data from multiple partitions to be combined (join, groupBy, distinct, reduceByKey)\n",
    "\n",
    "- Technique to minimize Data Shuffling:\n",
    "    - Repartition before shuffling: `df = df.partition(<number-of-partitions-OR-list-of-columns>)`\n",
    "    - Broadcast joins: `df_large.join(broadcast(df_small), \"key\")`\n",
    "    - Caching data: `df.cache()` (multiple joins for different tables)\n",
    "\n",
    "\n",
    "<img src=\"../img/data_distribution.png\" alt=\"Data distribution\" width=\"750\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ShuffleExample\").getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with sample data\n",
    "data = [\n",
    "    (1, \"Alice\", 100),\n",
    "    (2, \"Bob\", 200),\n",
    "    (3, \"Alice\", 150),\n",
    "    (4, \"Bob\", 250),\n",
    "    (5, \"Charlie\", 300)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"amount\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by \"name\" and sum the \"amount\" to force a shuffle\n",
    "grouped_df = df.groupBy(\"name\").sum(\"amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['name], ['name, sum(amount#2L) AS sum(amount)#10L]\n",
      "+- LogicalRDD [id#0L, name#1, amount#2L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "name: string, sum(amount): bigint\n",
      "Aggregate [name#1], [name#1, sum(amount#2L) AS sum(amount)#10L]\n",
      "+- LogicalRDD [id#0L, name#1, amount#2L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [name#1], [name#1, sum(amount#2L) AS sum(amount)#10L]\n",
      "+- Project [name#1, amount#2L]\n",
      "   +- LogicalRDD [id#0L, name#1, amount#2L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[name#1], functions=[sum(amount#2L)], output=[name#1, sum(amount)#10L])\n",
      "   +- Exchange hashpartitioning(name#1, 200), ENSURE_REQUIREMENTS, [plan_id=15]\n",
      "      +- HashAggregate(keys=[name#1], functions=[partial_sum(amount#2L)], output=[name#1, sum#14L])\n",
      "         +- Project [name#1, amount#2L]\n",
      "            +- Scan ExistingRDD[id#0L,name#1,amount#2L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use \"extended\" to get detailed plan\n",
    "grouped_df.explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   name|sum(amount)|\n",
      "+-------+-----------+\n",
      "|  Alice|        250|\n",
      "|    Bob|        450|\n",
      "|Charlie|        300|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trigger execution (e.g., show results)\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-pyspark-IpMtr_Fy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
