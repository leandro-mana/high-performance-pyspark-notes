{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Compression Techniques\n",
    "\n",
    "- **Serialization**:\n",
    "    - Turning data into binary\n",
    "    - Converting structured data, like Java objects or a Python dictionary into a compact binary format\n",
    "\n",
    "- **Deserialization**:\n",
    "    - Turning binary back into data\n",
    "    - Converting binary back into a structured data object\n",
    "\n",
    "- **Data Formats**:\n",
    "    - `CSV`:\n",
    "        - Raw text file with Comma-separated values\n",
    "        - Data type not stored\n",
    "        - Not storage efficient\n",
    "        - Good for small data sets, quick data exports\n",
    "    - `JSON`:\n",
    "        - JavaScript Object Notation, flexible format designed for modern web APIs, NoSQL and semi-structured data\n",
    "        - Data type not stored\n",
    "        - File size trend to be larger\n",
    "        - Querying in Spark requires extra parsing, which can slow performance\n",
    "    - `Avro`:\n",
    "        - Row-oriented data framework designed for efficiency, flexibility and cross-language compatibility\n",
    "        - Schema driven\n",
    "        - Compact and efficient\n",
    "        - Language neutral\n",
    "        - Splitable and compressible\n",
    "        - Schema evolution, handle data serialized with different versions of the schema without breaking compatibility (i.e. Kafka)\n",
    "    - `ORC`:\n",
    "        - Optimized Row Columnar\n",
    "        - built for speed up analytics workflows by storing data column by column\n",
    "        - Skips irrelevant data without reading\n",
    "        - Commonly used in Hadoop systems\n",
    "        - File Structured:\n",
    "            - Header\n",
    "            - Body: data stored in stripes, divided into index data, row data and stripe footer\n",
    "            - Footer: metadata about the file\n",
    "        - Considerations on, write performance and schema evolution\n",
    "    - `Parquet`:\n",
    "        - Used in big data storage formats like Apache Spark cloud data lakes, ML pipelines, columnar format like `ORC`\n",
    "        - Broad compatibility\n",
    "        - Used with Spark, Snowflake, AWS and Google BigQuery\n",
    "        - Optimized for reading, not writing\n",
    "        - Run-Length Encoding (RLE)\n",
    "        - Flexible schema evolution\n",
    "\n",
    "- **Compression Types**:\n",
    "    - `Zstandard` (ztsd), developed by Facebook, high compression ratios and fast speed, balance of efficiency and performance\n",
    "    - `Snappy`, developed by Google, prioritizes speed over compression ratio, good for real-time processing\n",
    "    - `LZ4` Developed by Yann Collet, its all about speed, ultra fast compression/decompression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from functools import wraps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame as PysparkDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CompressionBenchmark\").getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestCompression:\n",
    "    \"\"\"\n",
    "    Class to test compression and decompression benchmarks in Apache Spark.\n",
    "    \"\"\"\n",
    "    file_name: str\n",
    "    df: PysparkDataFrame\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def measure_execution_time(func):\n",
    "        \"\"\"\n",
    "        Decorator to measure function execution time\n",
    "        \"\"\"\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "\n",
    "            return result, execution_time\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "\n",
    "    @measure_execution_time\n",
    "    def test_compression(self, format: str, compression: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Test compression by writing a DataFrame to disk\n",
    "        \"\"\"\n",
    "        output_path = f\"output/{format}_{compression}\"\n",
    "\n",
    "        # Ensure output directory is empty before writing\n",
    "        if os.path.exists(output_path):\n",
    "            for file in os.listdir(output_path):\n",
    "                os.remove(os.path.join(output_path, file))\n",
    "\n",
    "        # Write DataFrame\n",
    "        self.df.write.mode(\"overwrite\").format(format).option(\"compression\", compression).save(output_path)\n",
    "\n",
    "        # Ensure directory exists before calculating size\n",
    "        file_size_mb = 0.0\n",
    "        if os.path.exists(output_path):\n",
    "            file_size = sum(\n",
    "                os.path.getsize(os.path.join(output_path, f))\n",
    "                for f in os.listdir(output_path) if os.path.isfile(os.path.join(output_path, f))\n",
    "            )\n",
    "            # Convert to MB\n",
    "            file_size_mb = round(file_size / (1024 * 1024), 2)\n",
    "\n",
    "        return file_size_mb\n",
    "\n",
    "\n",
    "    @measure_execution_time\n",
    "    def test_decompression(self, format: str, compression: str) -> None:\n",
    "        \"\"\"\n",
    "        Test decompression by reading a DataFrame from disk.\n",
    "        \"\"\"\n",
    "        input_path = f\"output/{format}_{compression}\"\n",
    "        _ = spark.read.format(format).load(input_path).count()\n",
    "\n",
    "\n",
    "    def test_compression_benchmarks(self) -> dict:\n",
    "        \"\"\"\n",
    "        Run compression and decompression benchmarks for Parquet using Zstd, Snappy, and LZ4.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        codecs = [\"zstd\", \"snappy\", \"lz4\"]\n",
    "\n",
    "        for codec in codecs:\n",
    "            # Test compression\n",
    "            size, compression_time = self.test_compression(\"parquet\", codec)\n",
    "            \n",
    "            # Test decompression\n",
    "            _, decompression_time = self.test_decompression(\"parquet\", codec)\n",
    "            \n",
    "            results[f\"parquet_{codec}\"] = {\n",
    "                \"size_mb\": size,\n",
    "                \"compression_time_seconds\": compression_time,\n",
    "                \"decompression_time_seconds\": decompression_time\n",
    "            }\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def get_original_file_size(self) -> float:\n",
    "        \"\"\"\n",
    "        Get the size of the original CSV file.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.file_name):\n",
    "            file_size = os.path.getsize(self.file_name)\n",
    "            return round(file_size / (1024 * 1024), 2)  # Convert to MB\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original CSV file size: 70.91 MB\n",
      "\n",
      "üî• Compression and Decompression Benchmark Results üî• \n",
      "\n",
      "parquet_zstd: \n",
      " \n",
      "  Compression Time: 0.52s\n",
      "  Decompression Time: 0.09s\n",
      "  Size: 10.94 MB\n",
      "\n",
      "parquet_snappy: \n",
      " \n",
      "  Compression Time: 0.51s\n",
      "  Decompression Time: 0.07s\n",
      "  Size: 22.77 MB\n",
      "\n",
      "parquet_lz4: \n",
      " \n",
      "  Compression Time: 0.49s\n",
      "  Decompression Time: 0.08s\n",
      "  Size: 23.14 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to CSV file\n",
    "FILE_PATH = \"../data/test_data.csv\"\n",
    "\n",
    "# Read CSV into DataFrame with correct data types\n",
    "df = spark.read.option(\"header\", \"true\").csv(FILE_PATH)\n",
    "\n",
    "# Ensure DataFrame is not empty\n",
    "if df.count() == 0:\n",
    "    print(\"‚ùå ERROR: DataFrame is empty. Please check the input CSV file.\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Get original file size before compression\n",
    "test_compression = TestCompression(file_name=FILE_PATH, df=df)\n",
    "original_size_mb = test_compression.get_original_file_size()\n",
    "\n",
    "# Run compression and decompression benchmarks\n",
    "results = test_compression.test_compression_benchmarks()\n",
    "\n",
    "# Print original file size and benchmark results\n",
    "print(f\"\\nOriginal CSV file size: {original_size_mb} MB\\n\")\n",
    "print(\"üî• Compression and Decompression Benchmark Results üî• \\n\")\n",
    "for algorithm, metrics in results.items():\n",
    "    print(f\"{algorithm}: \\n \")\n",
    "    print(f\"  Compression Time: {metrics['compression_time_seconds']:.2f}s\")\n",
    "    print(f\"  Decompression Time: {metrics['decompression_time_seconds']:.2f}s\")\n",
    "    print(f\"  Size: {metrics['size_mb']} MB\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-pyspark-IpMtr_Fy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
