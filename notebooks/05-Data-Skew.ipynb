{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Skew\n",
    "\n",
    "Its one of the most common causes of data shuffle perfomance issues, it occurs when some partitions have much more data than others\n",
    "leading to slow-running tasks, disk spills and Out-Of-Memory errors\n",
    "\n",
    "Partitions are chunks of data distributed and processed by worker nodes\n",
    "\n",
    "<img src=\"../img/spark_cluster.png\" alt=\"Cluster\" width=\"650\">\n",
    "\n",
    "- Correct Data Skew:\n",
    "    - Repartition: `df = df.partition(<number-of-partitions-OR-list-of-columns>)`\n",
    "    - Add a \"salt\" column\n",
    "\n",
    "```Python\n",
    "# Add a 'salt' column with a random value for each row\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = df.withColumn(\"salt\", F.rand())\n",
    "\n",
    "# Replartition the DataFrame into 8 partitions based on the 'salt' column\n",
    "df = df.repartition(8, \"salt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set up PySpark Session\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataSkewHandling\").getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  1|       A|\n",
      "|  1|       A|\n",
      "|  1|       A|\n",
      "|  1|       A|\n",
      "|  1|       A|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create a Sample DataFrame with skewed data\n",
    "data = [(1, \"A\")] * 1000 + [(2, \"B\")] * 100 + [(3, \"C\")] * 10\n",
    "df = spark.createDataFrame(data, [\"id\", \"category\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows per partition:\n",
      "+--------------------+-----+\n",
      "|SPARK_PARTITION_ID()|count|\n",
      "+--------------------+-----+\n",
      "|                   0|   92|\n",
      "|                   1|   92|\n",
      "|                   2|   92|\n",
      "|                   3|   92|\n",
      "|                   4|   92|\n",
      "|                   5|   92|\n",
      "|                   6|   92|\n",
      "|                   7|   92|\n",
      "|                   8|   92|\n",
      "|                   9|   92|\n",
      "|                  10|   92|\n",
      "|                  11|   98|\n",
      "+--------------------+-----+\n",
      "\n",
      "\n",
      "Data in partitions (first 2 rows per partition):\n",
      "Partition 0: [Row(id=1, category='A'), Row(id=1, category='A')]\n",
      "Partition 1: [Row(id=1, category='A'), Row(id=1, category='A')]\n",
      "Partition 2: [Row(id=1, category='A'), Row(id=1, category='A')]\n",
      "Partition 3: [Row(id=1, category='A'), Row(id=1, category='A')]\n",
      "Partition 4: [Row(id=1, category='A'), Row(id=1, category='A')]\n",
      "Partition 5: [Row(id=1, category='A'), Row(id=1, category='A')]\n",
      "Partition 6: [Row(id=1, category='A'), Row(id=1, category='A')]\n",
      "Partition 7: [Row(id=1, category='A'), Row(id=1, category='A')]\n",
      "Partition 8: [Row(id=1, category='A'), Row(id=1, category='A')]\n",
      "Partition 9: [Row(id=1, category='A'), Row(id=1, category='A')]\n",
      "Partition 10: [Row(id=1, category='A'), Row(id=1, category='A')]\n",
      "Partition 11: [Row(id=2, category='B'), Row(id=2, category='B')]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Diagnose Data Skew\n",
    "\n",
    "# Check the number of rows per partition\n",
    "print(\"\\nNumber of rows per partition:\")\n",
    "df.groupBy(F.spark_partition_id()).count().show()\n",
    "\n",
    "# Inspect data distribution in partitions\n",
    "print(\"\\nData in partitions (first 2 rows per partition):\")\n",
    "partitions = df.rdd.glom().collect()\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f\"Partition {i}: {partition[:2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+-----+\n",
      "|partition_id| id|count|\n",
      "+------------+---+-----+\n",
      "|           0|  1|   92|\n",
      "|           1|  1|   92|\n",
      "|           2|  1|   92|\n",
      "|           3|  1|   92|\n",
      "|           4|  1|   92|\n",
      "|           5|  1|   92|\n",
      "|           6|  1|   92|\n",
      "|           7|  1|   92|\n",
      "|           8|  1|   92|\n",
      "|           9|  1|   92|\n",
      "|          10|  1|   80|\n",
      "|          10|  2|   12|\n",
      "|          11|  2|   88|\n",
      "|          11|  3|   10|\n",
      "+------------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of 'id' across partitions\n",
    "df.withColumn(\"partition_id\", F.spark_partition_id()) \\\n",
    "  .groupBy(\"partition_id\", \"id\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"partition_id\", \"id\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Repartitioning by 'id' column...\n",
      "\n",
      "Number of rows per partition after repartitioning:\n",
      "+--------------------+-----+\n",
      "|SPARK_PARTITION_ID()|count|\n",
      "+--------------------+-----+\n",
      "|                   0| 1110|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Handle Data Skew - Repartition by Column\n",
    "\n",
    "# Repartition the DataFrame by the skewed column\n",
    "print(\"\\nRepartitioning by 'id' column...\")\n",
    "df_repartitioned = df.repartition(\"id\")\n",
    "\n",
    "# Check the new distribution\n",
    "print(\"\\nNumber of rows per partition after repartitioning:\")\n",
    "df_repartitioned.groupBy(F.spark_partition_id()).count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding a salt column for even distribution...\n",
      "\n",
      "Number of rows per partition after salting:\n",
      "+--------------------+-----+\n",
      "|SPARK_PARTITION_ID()|count|\n",
      "+--------------------+-----+\n",
      "|                   0|  116|\n",
      "|                   1|  142|\n",
      "|                   2|  152|\n",
      "|                   3|  132|\n",
      "|                   4|  132|\n",
      "|                   5|  159|\n",
      "|                   6|  130|\n",
      "|                   7|  147|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Handle Data Skew - Salting\n",
    "\n",
    "# Add a salt column to evenly distribute data\n",
    "print(\"\\nAdding a salt column for even distribution...\")\n",
    "df_salted = df.withColumn(\"salt\", F.rand())\n",
    "\n",
    "# Repartition by the salt column\n",
    "df_salted = df_salted.repartition(8, \"salt\")\n",
    "\n",
    "# Check the new distribution\n",
    "print(\"\\nNumber of rows per partition after salting:\")\n",
    "df_salted.groupBy(F.spark_partition_id()).count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-pyspark-IpMtr_Fy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
