{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# NOTE: On a notebook is preferable to have the imports first and then the Spark Session block\n",
    "# so in case of adding more libraries to import, than can be executed any time, while the Session just once\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    when,\n",
    "    isnull,\n",
    "    count,\n",
    "    split,\n",
    "    lit,\n",
    "    abs,\n",
    "    round,\n",
    "    regexp_extract,\n",
    "    regexp_replace,\n",
    "    array_remove,\n",
    "    explode,\n",
    "    to_date,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkDataClean\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to WARN to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for the dataset\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_details\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"quantity\", StringType(), True),\n",
    "    StructField(\"price_per_unit\", StringType(), True),\n",
    "    StructField(\"tags\", StringType(), True),\n",
    "    StructField(\"items\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv(\"../data/online_sales_data.csv\", schema=schema, header=True)\n",
    "\n",
    "# Display the dataset\n",
    "print(\"Raw Dataset:\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all the NULLs in the dataframe, defining a function so it can be reused on the Notebook\n",
    "# NOTE: .alias(c) to get the name of the column in the header\n",
    "\n",
    "def get_all_nulls(df: SparkDataFrame) -> SparkDataFrame:\n",
    "    \"\"\"\n",
    "    This function will return a DataFrame with all the Null\n",
    "    values per column\n",
    "    \"\"\"\n",
    "    result_df = df.select(\n",
    "        [count(when(isnull(c), c)).alias(c) for c in df.columns]\n",
    "    )\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Usage\n",
    "all_nulls_df = get_all_nulls(df=df)\n",
    "all_nulls_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect rows with negative quantity or invalid price\n",
    "df.filter(\n",
    "    (col(\"quantity\") < 0) |\n",
    "    (col(\"quantity\").rlike(\"^[^0-9]\")) | \n",
    "    (col(\"price_per_unit\").rlike(\"^[^0-9]\"))\n",
    ").select(\"quantity\", \"price_per_unit\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As noted, both columns, \"price_per_unit\" and \"quantity\" have corrupted data, that imply NULL, Negative Numers and StringsType (Schema)\n",
    "# So as a first step, lets try to identify all the wrong values we need to fix in \"quantity\"\n",
    "\n",
    "unique_quantity_values = df.filter(\n",
    "    (col(\"quantity\") < 0) |\n",
    "    (col(\"quantity\").rlike(r\"^[0-9]+$\")) |\n",
    "    (col(\"quantity\").rlike(r\"^[a-z]+$\")) |\n",
    "    (col(\"quantity\").isNull())\n",
    ").select(\"quantity\").distinct().rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "print(f\"Unique quantity col values: {len(unique_quantity_values)}\")\n",
    "print(sorted(unique_quantity_values, key=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the result above we can see that if we cast() values we will loose any of the Null (None) and the \"ten\"\n",
    "# So the basic rules to fix the data on this column could be:\n",
    "#   - replace \"ten\" by \"10\"\n",
    "#   - replace Null by \"0\"\n",
    "#   - then cast the quantity column values to INT and replace negative for absolute values\n",
    "df = df.withColumn(\"quantity\", when(col(\"quantity\").isNull(), 0).otherwise(col(\"quantity\")))\n",
    "df = df.withColumn(\"quantity\", when(col(\"quantity\") == \"ten\", 10).otherwise(col(\"quantity\")))\n",
    "df = df.withColumn(\"quantity\", col(\"quantity\").cast(IntegerType()))\n",
    "df = df.withColumn(\"quantity\", abs(df[\"quantity\"]))\n",
    "\n",
    "# Check the changes\n",
    "unique_quantity_clean_values = df.select(\"quantity\").distinct().rdd.map(lambda x: x[0]).collect()\n",
    "print(f\"Unique quantity col values: {len(unique_quantity_clean_values)}\")\n",
    "print(sorted(unique_quantity_clean_values, key=int))\n",
    "df.select(\"quantity\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again Nulls, now \"quantity\" Column has been fixed, based on the 22 original values\n",
    "all_nulls_df = get_all_nulls(df=df)\n",
    "all_nulls_df.select(\"quantity\", \"price_per_unit\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply the same principle but now on the \"price_per_unit\" column\n",
    "# but in this case, besides replacing string values for integers, and negative for absolute\n",
    "# will also imply replacing Nulls by the Median value of the column values as distribution, 50th percentile\n",
    "# using the Median instead of the average is better in this cases as the Median is not impacted by edge/extreme values\n",
    "# Data cleaning actions - continuing with the df DataFrame:\n",
    "#   - Cast to double\n",
    "#   - Round with 2 decimal digits\n",
    "#   - Replace Null by the Median value, using ALL column values as the distribution \n",
    "\n",
    "# Lets get the unique values to know which literals to replace\n",
    "unique_ppu_values = df.filter(\n",
    "    (col(\"price_per_unit\") < 0) |\n",
    "    (col(\"price_per_unit\").rlike(r\"^[0-9]+$\")) |\n",
    "    (col(\"price_per_unit\").rlike(r\"^[a-z]+$\")) |\n",
    "    (col(\"price_per_unit\").isNull())\n",
    ").select(\"price_per_unit\").distinct().rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "print(f\"Unique quantity col values: {len(unique_ppu_values)}\")\n",
    "print(sorted(unique_ppu_values, key=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check if besides Null values, there are \"0\" as value on \"price_per_unit\" Column\n",
    "df.filter((col(\"price_per_unit\") == \"0\")).select(\"price_per_unit\").show()\n",
    "\n",
    "# As there are no \"0\" values already, then its safe to replace each NULL for 0.0 given the column type in the context of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace literals and Null\n",
    "df = df.withColumn(\n",
    "    \"price_per_unit\", when(col(\"price_per_unit\").isNull(), \"0\").otherwise(col(\"price_per_unit\"))\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"price_per_unit\", when(col(\"price_per_unit\") == \"fifty\", \"50.00\").otherwise(col(\"price_per_unit\"))\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"price_per_unit\", col(\"price_per_unit\").cast(DoubleType()))\n",
    "median_from_col_quantity = df.approxQuantile(\"price_per_unit\", [0.5], 0.0)[0]\n",
    "print(f\"Median value of price_per_unit column: {median_from_col_quantity}\")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"price_per_unit\",\n",
    "    when(col(\"price_per_unit\") == 0, median_from_col_quantity).otherwise(col(\"price_per_unit\"))\n",
    ")\n",
    "# Round to 2 decimal places for \"price_per_unit\"\n",
    "df = df.withColumn(\"price_per_unit\", round(col(\"price_per_unit\"), 2))\n",
    "\n",
    "df.select(\"price_per_unit\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check two specific orders that should have now the rounded numbers\n",
    "df.filter((col(\"order_id\") == \"ORD001\") | (col(\"order_id\") == \"ORD003\")).select(\"order_id\", \"price_per_unit\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again Nulls, now \"quantity\" and \"price_per_unit\" Columns have been fixed\n",
    "all_nulls_df = get_all_nulls(df=df)\n",
    "all_nulls_df.select(\"quantity\", \"price_per_unit\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets fix \"product_category\" as it has 30 Nulls, so lets check that only NULL is what is missing and set for unknown_category\n",
    "unique_prod_category_values = df.filter(\n",
    "    (col(\"product_category\").rlike(r\"^[a-z]\")) |\n",
    "    (col(\"product_category\").rlike(r\"^[A-Z]\")) |\n",
    "    (col(\"product_category\").isNull())\n",
    ").select(\"product_category\").distinct().rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "print(f\"Unique product_category col values: {len(unique_prod_category_values)}\")\n",
    "print(sorted(unique_prod_category_values, key=str))\n",
    "\n",
    "# Replacing Nulls\n",
    "df = df.withColumn(\n",
    "    \"product_category\", when(col(\"product_category\").isNull(), \"unknown_category\").otherwise(col(\"product_category\"))\n",
    ")\n",
    "\n",
    "df.select(\"product_category\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now based on the \"customer_details\" column, we can split such information by \"|\" and create the following:\n",
    "#   - customer_name: split by | and get the first set of values (index 0)\n",
    "#   - customer_address: split by | and get the second set of values (index 1)\n",
    "#   - customer_address: replace NULL by \"unknown_address\"\n",
    "df = df.withColumn(\"customer_name\", split(col(\"customer_details\"), \"\\\\|\")[0])\n",
    "df = df.withColumn(\"customer_address\", split(col(\"customer_details\"), \"\\\\|\")[1])\n",
    "df = df.withColumn(\n",
    "    \"customer_address\", \n",
    "    when(col(\"customer_address\").isNull(), lit(\"unknown_address\")).otherwise(col(\"customer_address\"))\n",
    ")\n",
    "\n",
    "df = df.drop(\"customer_details\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check an example of customer_address, it contains Street Name and City, so we can create two more columns from it\n",
    "df.filter(col(\"order_id\") == \"ORD002\").select(\"customer_address\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on above cell, extract components from customer_address:\n",
    "#   - street_name: extract \"Street Name\"\n",
    "#   - city: extract \"City\"\n",
    "df = df.withColumn(\"street\", regexp_extract(col('customer_address'), r'(\\d+) Street Name', 1))\n",
    "df = df.withColumn(\"city\", regexp_extract(col('customer_address'), r'City (\\d+)', 1))\n",
    "df.select(\"customer_address\", \"street\", \"city\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"unknown\" for the empty string values on each column, and drop \"customer_address\" column\n",
    "df = df.withColumn(\n",
    "    \"street\", \n",
    "    when(col(\"street\") == \"\", lit(\"unknown\")).otherwise(col(\"street\"))\n",
    ") \\\n",
    ".withColumn(\n",
    "    \"city\", \n",
    "    when(col(\"city\") == \"\", lit(\"unknown\")).otherwise(col(\"city\"))\n",
    ")\n",
    "\n",
    "df = df.drop(\"customer_address\")\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets fix \"items\" and \"tags\" columns, to set column type to Array, to do this first we need to convert the literals\n",
    "# \"['value']\" which is an string, into a real list, using the split function\n",
    "\n",
    "# Create idem-potent function to reuse for both columns that needs similar transformation\n",
    "def transform_col_to_array_type(df: SparkDataFrame, col_name: str) -> SparkDataFrame:\n",
    "    \"\"\"\n",
    "    This function will transform a column with StringType into ArrayType(StringType)\n",
    "    also replacing NULL for empty string, in an idem-potent way\n",
    "    \"\"\"\n",
    "    if not dict(df.dtypes)[col_name] == \"array<string>\":\n",
    "        df = df.withColumn(col_name, regexp_replace(col(col_name), r\"^\\[\", \"\")) \\\n",
    "            .withColumn(col_name, regexp_replace(col(col_name), r\"\\]$\", \"\")) \\\n",
    "            .withColumn(col_name, regexp_replace(col(col_name), r\"'\", \"\"))\n",
    "\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name).isNull(), \"\").otherwise(col(col_name))\n",
    "        )\n",
    "\n",
    "        df = df.withColumn(col_name, split(col(col_name), \", \"))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = transform_col_to_array_type(df=df, col_name=\"tags\")\n",
    "df.select(\"tags\").show(10, False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same transformation process for \"items\"\n",
    "df = transform_col_to_array_type(df=df, col_name=\"items\")\n",
    "df.select(\"items\").show(10, False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see there is still work to be done on the \"items\" column as on each row, inside the arrays\n",
    "# there are values like \"-1\", \"None\" that are also corrupting the data, so lets try to identify\n",
    "# if besides those two there are other values that need to be removed\n",
    "\n",
    "# Get a unique set of values by getting the set() of everything that is not \"None\" AND \"-1\"\n",
    "# explode() and collect() used here\n",
    "result = list()\n",
    "for items in df.select(\"items\", explode(\"items\")).collect():\n",
    "    result.extend([item for item in items[0] if not item == \"None\" and not item == \"-1\"])\n",
    "\n",
    "list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So based on the result above, we are confident that only \"None\" and \"-1\" are the values\n",
    "# to be removed for each array when present. Using array_remove() for this\n",
    "df = df.withColumn(\"items\", array_remove(\"items\", \"None\")).withColumn(\"items\", array_remove(\"items\", \"-1\"))\n",
    "df.select(\"items\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check how the DataSet is now afer the transformations\n",
    "df.show(10, truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets fill NULLs for a default date of \"1900-01-01\", then set the column to_date()\n",
    "df = df.withColumn(\"order_date\", when(col(\"order_date\").isNull(), lit(\"1900-01-01\")).otherwise(col(\"order_date\")))\n",
    "df = df.withColumn(\"order_date\", to_date(\"order_date\", \"yyyy-MM-dd\"))\n",
    "df.select(\"order_date\").show(5, truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End result\n",
    "df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- When initially loading sample data (CSV, JSON) to define its quality, using an Schema based on StringType is a safe approach\n",
    "- Then by checking each column to infer which is the correct type to use, check which minimal transformation needs to be done\n",
    "- Casting directly a column to the desired type might lead to loose data that could be trasformed prior infering the Type\n",
    "- In some cases the context of the data in a column will allow to replace missing/corrupted data, like None, Null, -1, etc\n",
    "- Dig into PySpark SQL Functions in order to interact with the data in a more performant and safer way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-pyspark-IpMtr_Fy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
