{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# NOTE: On a notebook is preferable to have the imports first and then the Spark Session block\n",
    "# so in case of adding more libraries to import, than can be executed any time, while the Session just once\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    when,\n",
    "    isnull,\n",
    "    count,\n",
    "    split,\n",
    "    lit,\n",
    "    abs,\n",
    "    round,\n",
    "    regexp_extract,\n",
    "    regexp_replace,\n",
    "    array_remove,\n",
    "    explode,\n",
    "    to_date,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkDataClean\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Dataset:\n",
      "+--------+-----------------------------------------+----------+----------------+--------+------------------+------------------+----------------------------+\n",
      "|order_id|customer_details                         |order_date|product_category|quantity|price_per_unit    |tags              |items                       |\n",
      "+--------+-----------------------------------------+----------+----------------+--------+------------------+------------------+----------------------------+\n",
      "|ORD001  |Alice Johnson                            |NULL      |Electronics     |4       |15.769160684603047|['urgent', 'gift']|['Phone', 'Charger', 'Case']|\n",
      "|ORD002  |Bob Smith | 584 Street Name, City 16     |2022-12-30|NULL            |-3      |fifty             |['bulk_order']    |['Book1', 'Book2']          |\n",
      "|ORD003  |Charlie Brown | 598 Street Name, City 17 |2023-05-22|Books           |ten     |79.63563178465238 |NULL              |['Book1', 'Book2']          |\n",
      "|ORD004  |David Wilson | 290 Street Name, City 12  |2022-12-30|Books           |NULL    |27.556430196566655|NULL              |['Laptop', 'Mouse', None]   |\n",
      "|ORD005  |Eva Davis | 387 Street Name, City 10     |NULL      |NULL            |-4      |fifty             |['bulk_order']    |['Phone', 'Charger', 'Case']|\n",
      "|ORD006  |Frank Miller                             |2023-05-22|NULL            |10      |fifty             |urgent,gift       |['Table', 'Chair', -1]      |\n",
      "|ORD007  |Grace Lee | 869 Street Name, City 16     |2023-05-22|Books           |NULL    |fifty             |['bulk_order']    |['Table', 'Chair', -1]      |\n",
      "|ORD008  |Henry Moore | 661 Street Name, City 7    |2022-12-30|NULL            |-3      |NULL              |urgent,gift       |['Table', 'Chair', -1]      |\n",
      "|ORD009  |Ivy Taylor | 239 Street Name, City 3     |NULL      |NULL            |NULL    |fifty             |urgent,gift       |['Book1', 'Book2']          |\n",
      "|ORD010  |James Anderson | 760 Street Name, City 16|2023-01-15|Electronics     |4       |37.530252520104796|urgent,gift       |NULL                        |\n",
      "+--------+-----------------------------------------+----------+----------------+--------+------------------+------------------+----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema for the dataset\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_details\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"quantity\", StringType(), True),\n",
    "    StructField(\"price_per_unit\", StringType(), True),\n",
    "    StructField(\"tags\", StringType(), True),\n",
    "    StructField(\"items\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv(\"../data/online_sales_data.csv\", schema=schema, header=True)\n",
    "\n",
    "# Display the dataset\n",
    "print(\"Raw Dataset:\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+----------+----------------+-----------------+------------------+--------------+--------------------+\n",
      "|summary|order_id|    customer_details|order_date|product_category|         quantity|    price_per_unit|          tags|               items|\n",
      "+-------+--------+--------------------+----------+----------------+-----------------+------------------+--------------+--------------------+\n",
      "|  count|     100|                 100|        63|              70|               78|                62|            82|                  80|\n",
      "|   mean|    NULL|                NULL|      NULL|            NULL|             1.68| 55.49113068483046|          NULL|                NULL|\n",
      "| stddev|    NULL|                NULL|      NULL|            NULL|4.639845176868095|27.456621254927434|          NULL|                NULL|\n",
      "|    min|  ORD001|       Alice Johnson|2022-12-30|           Books|               -1|15.769160684603047|['bulk_order']|  ['Book1', 'Book2']|\n",
      "|    max|  ORD100|James Anderson | ...|2023-05-22|  Home & Kitchen|              ten|             fifty|   urgent,gift|['Table', 'Chair'...|\n",
      "+-------+--------+--------------------+----------+----------------+-----------------+------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------+----------------+--------+--------------+----+-----+\n",
      "|order_id|customer_details|order_date|product_category|quantity|price_per_unit|tags|items|\n",
      "+--------+----------------+----------+----------------+--------+--------------+----+-----+\n",
      "|       0|               0|        37|              30|      22|            38|  18|   20|\n",
      "+--------+----------------+----------+----------------+--------+--------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show all the NULLs in the dataframe, defining a function so it can be reused on the Notebook\n",
    "# NOTE: .alias(c) to get the name of the column in the header\n",
    "\n",
    "def get_all_nulls(df: SparkDataFrame) -> SparkDataFrame:\n",
    "    \"\"\"\n",
    "    This function will return a DataFrame with all the Null\n",
    "    values per column\n",
    "    \"\"\"\n",
    "    result_df = df.select(\n",
    "        [count(when(isnull(c), c)).alias(c) for c in df.columns]\n",
    "    )\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Usage\n",
    "all_nulls_df = get_all_nulls(df=df)\n",
    "all_nulls_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_details: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- price_per_unit: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- items: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|quantity|    price_per_unit|\n",
      "+--------+------------------+\n",
      "|      -3|             fifty|\n",
      "|     ten| 79.63563178465238|\n",
      "|      -4|             fifty|\n",
      "|      10|             fifty|\n",
      "|    NULL|             fifty|\n",
      "|      -3|              NULL|\n",
      "|    NULL|             fifty|\n",
      "|      -4| 26.15555520916483|\n",
      "|    NULL|             fifty|\n",
      "|       3|             fifty|\n",
      "|     ten|             fifty|\n",
      "|      -3|             fifty|\n",
      "|      -2| 19.55396781305644|\n",
      "|       7|             fifty|\n",
      "|      -4|             fifty|\n",
      "|     ten|19.030281187517616|\n",
      "|     ten|              NULL|\n",
      "|     ten|             fifty|\n",
      "|      -1|             fifty|\n",
      "|      -2|              NULL|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detect rows with negative quantity or invalid price\n",
    "df.filter(\n",
    "    (col(\"quantity\") < 0) |\n",
    "    (col(\"quantity\").rlike(\"^[^0-9]\")) | \n",
    "    (col(\"price_per_unit\").rlike(\"^[^0-9]\"))\n",
    ").select(\"quantity\", \"price_per_unit\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique quantity col values: 16\n",
      "['-1', '-2', '-3', '-4', '1', '10', '2', '3', '4', '5', '6', '7', '8', '9', None, 'ten']\n"
     ]
    }
   ],
   "source": [
    "# As noted, both columns, \"price_per_unit\" and \"quantity\" have corrupted data, that imply NULL, Negative Numers and StringsType (Schema)\n",
    "# So as a first step, lets try to identify all the wrong values we need to fix in \"quantity\"\n",
    "\n",
    "unique_quantity_values = df.filter(\n",
    "    (col(\"quantity\") < 0) |\n",
    "    (col(\"quantity\").rlike(r\"^[0-9]+$\")) |\n",
    "    (col(\"quantity\").rlike(r\"^[a-z]+$\")) |\n",
    "    (col(\"quantity\").isNull())\n",
    ").select(\"quantity\").distinct().rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "print(f\"Unique quantity col values: {len(unique_quantity_values)}\")\n",
    "print(sorted(unique_quantity_values, key=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique quantity col values: 11\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "+--------+\n",
      "|quantity|\n",
      "+--------+\n",
      "|       4|\n",
      "|       3|\n",
      "|      10|\n",
      "|       0|\n",
      "|       4|\n",
      "|      10|\n",
      "|       0|\n",
      "|       3|\n",
      "|       0|\n",
      "|       4|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Based on the result above we can see that if we cast() values we will loose any of the Null (None) and the \"ten\"\n",
    "# So the basic rules to fix the data on this column could be:\n",
    "#   - replace \"ten\" by \"10\"\n",
    "#   - replace Null by \"0\"\n",
    "#   - then cast the quantity column values to INT and replace negative for absolute values\n",
    "df = df.withColumn(\"quantity\", when(col(\"quantity\").isNull(), 0).otherwise(col(\"quantity\")))\n",
    "df = df.withColumn(\"quantity\", when(col(\"quantity\") == \"ten\", 10).otherwise(col(\"quantity\")))\n",
    "df = df.withColumn(\"quantity\", col(\"quantity\").cast(IntegerType()))\n",
    "df = df.withColumn(\"quantity\", abs(df[\"quantity\"]))\n",
    "\n",
    "# Check the changes\n",
    "unique_quantity_clean_values = df.select(\"quantity\").distinct().rdd.map(lambda x: x[0]).collect()\n",
    "print(f\"Unique quantity col values: {len(unique_quantity_clean_values)}\")\n",
    "print(sorted(unique_quantity_clean_values, key=int))\n",
    "df.select(\"quantity\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|quantity|price_per_unit|\n",
      "+--------+--------------+\n",
      "|       0|            38|\n",
      "+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check again Nulls, now \"quantity\" Column has been fixed, based on the 22 original values\n",
    "all_nulls_df = get_all_nulls(df=df)\n",
    "all_nulls_df.select(\"quantity\", \"price_per_unit\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique quantity col values: 2\n",
      "[None, 'fifty']\n"
     ]
    }
   ],
   "source": [
    "# We can apply the same principle but now on the \"price_per_unit\" column\n",
    "# but in this case, besides replacing string values for integers, and negative for absolute\n",
    "# will also imply replacing Nulls by the Median value of the column values as distribution, 50th percentile\n",
    "# using the Median instead of the average is better in this cases as the Median is not impacted by edge/extreme values\n",
    "# Data cleaning actions - continuing with the df DataFrame:\n",
    "#   - Cast to double\n",
    "#   - Round with 2 decimal digits\n",
    "#   - Replace Null by the Median value, using ALL column values as the distribution \n",
    "\n",
    "# Lets get the unique values to know which literals to replace\n",
    "unique_ppu_values = df.filter(\n",
    "    (col(\"price_per_unit\") < 0) |\n",
    "    (col(\"price_per_unit\").rlike(r\"^[0-9]+$\")) |\n",
    "    (col(\"price_per_unit\").rlike(r\"^[a-z]+$\")) |\n",
    "    (col(\"price_per_unit\").isNull())\n",
    ").select(\"price_per_unit\").distinct().rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "print(f\"Unique quantity col values: {len(unique_ppu_values)}\")\n",
    "print(sorted(unique_ppu_values, key=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|price_per_unit|\n",
      "+--------------+\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets check if besides Null values, there are \"0\" as value on \"price_per_unit\" Column\n",
    "df.filter((col(\"price_per_unit\") == \"0\")).select(\"price_per_unit\").show()\n",
    "\n",
    "# As there are no \"0\" values already, then its safe to replace each NULL for 0.0 given the column type in the context of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median value of price_per_unit column: 43.861076396177964\n",
      "+--------------+\n",
      "|price_per_unit|\n",
      "+--------------+\n",
      "|         15.77|\n",
      "|          50.0|\n",
      "|         79.64|\n",
      "|         27.56|\n",
      "|          50.0|\n",
      "|          50.0|\n",
      "|          50.0|\n",
      "|         43.86|\n",
      "|          50.0|\n",
      "|         37.53|\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace literals and Null\n",
    "df = df.withColumn(\n",
    "    \"price_per_unit\", when(col(\"price_per_unit\").isNull(), \"0\").otherwise(col(\"price_per_unit\"))\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"price_per_unit\", when(col(\"price_per_unit\") == \"fifty\", \"50.00\").otherwise(col(\"price_per_unit\"))\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"price_per_unit\", col(\"price_per_unit\").cast(DoubleType()))\n",
    "median_from_col_quantity = df.approxQuantile(\"price_per_unit\", [0.5], 0.0)[0]\n",
    "print(f\"Median value of price_per_unit column: {median_from_col_quantity}\")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"price_per_unit\",\n",
    "    when(col(\"price_per_unit\") == 0, median_from_col_quantity).otherwise(col(\"price_per_unit\"))\n",
    ")\n",
    "# Round to 2 decimal places for \"price_per_unit\"\n",
    "df = df.withColumn(\"price_per_unit\", round(col(\"price_per_unit\"), 2))\n",
    "\n",
    "df.select(\"price_per_unit\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|order_id|price_per_unit|\n",
      "+--------+--------------+\n",
      "|  ORD001|         15.77|\n",
      "|  ORD003|         79.64|\n",
      "+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check two specific orders that should have now the rounded numbers\n",
    "df.filter((col(\"order_id\") == \"ORD001\") | (col(\"order_id\") == \"ORD003\")).select(\"order_id\", \"price_per_unit\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|quantity|price_per_unit|\n",
      "+--------+--------------+\n",
      "|       0|             0|\n",
      "+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check again Nulls, now \"quantity\" and \"price_per_unit\" Columns have been fixed\n",
    "all_nulls_df = get_all_nulls(df=df)\n",
    "all_nulls_df.select(\"quantity\", \"price_per_unit\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique product_category col values: 4\n",
      "['Books', 'Electronics', 'Home & Kitchen', None]\n",
      "+----------------+\n",
      "|product_category|\n",
      "+----------------+\n",
      "|     Electronics|\n",
      "|unknown_category|\n",
      "|           Books|\n",
      "|           Books|\n",
      "|unknown_category|\n",
      "|unknown_category|\n",
      "|           Books|\n",
      "|unknown_category|\n",
      "|unknown_category|\n",
      "|     Electronics|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now lets fix \"product_category\" as it has 30 Nulls, so lets check that only NULL is what is missing and set for unknown_category\n",
    "unique_prod_category_values = df.filter(\n",
    "    (col(\"product_category\").rlike(r\"^[a-z]\")) |\n",
    "    (col(\"product_category\").rlike(r\"^[A-Z]\")) |\n",
    "    (col(\"product_category\").isNull())\n",
    ").select(\"product_category\").distinct().rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "print(f\"Unique product_category col values: {len(unique_prod_category_values)}\")\n",
    "print(sorted(unique_prod_category_values, key=str))\n",
    "\n",
    "# Replacing Nulls\n",
    "df = df.withColumn(\n",
    "    \"product_category\", when(col(\"product_category\").isNull(), \"unknown_category\").otherwise(col(\"product_category\"))\n",
    ")\n",
    "\n",
    "df.select(\"product_category\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------+--------+--------------+------------------+--------------------+---------------+--------------------+\n",
      "|order_id|order_date|product_category|quantity|price_per_unit|              tags|               items|  customer_name|    customer_address|\n",
      "+--------+----------+----------------+--------+--------------+------------------+--------------------+---------------+--------------------+\n",
      "|  ORD001|      NULL|     Electronics|       4|         15.77|['urgent', 'gift']|['Phone', 'Charge...|  Alice Johnson|     unknown_address|\n",
      "|  ORD002|2022-12-30|unknown_category|       3|          50.0|    ['bulk_order']|  ['Book1', 'Book2']|     Bob Smith | 584 Street Name,...|\n",
      "|  ORD003|2023-05-22|           Books|      10|         79.64|              NULL|  ['Book1', 'Book2']| Charlie Brown | 598 Street Name,...|\n",
      "|  ORD004|2022-12-30|           Books|       0|         27.56|              NULL|['Laptop', 'Mouse...|  David Wilson | 290 Street Name,...|\n",
      "|  ORD005|      NULL|unknown_category|       4|          50.0|    ['bulk_order']|['Phone', 'Charge...|     Eva Davis | 387 Street Name,...|\n",
      "|  ORD006|2023-05-22|unknown_category|      10|          50.0|       urgent,gift|['Table', 'Chair'...|   Frank Miller|     unknown_address|\n",
      "|  ORD007|2023-05-22|           Books|       0|          50.0|    ['bulk_order']|['Table', 'Chair'...|     Grace Lee | 869 Street Name,...|\n",
      "|  ORD008|2022-12-30|unknown_category|       3|         43.86|       urgent,gift|['Table', 'Chair'...|   Henry Moore | 661 Street Name,...|\n",
      "|  ORD009|      NULL|unknown_category|       0|          50.0|       urgent,gift|  ['Book1', 'Book2']|    Ivy Taylor | 239 Street Name,...|\n",
      "|  ORD010|2023-01-15|     Electronics|       4|         37.53|       urgent,gift|                NULL|James Anderson | 760 Street Name,...|\n",
      "|  ORD011|      NULL|unknown_category|       4|         26.16|       urgent,gift|                NULL|  Alice Johnson|     unknown_address|\n",
      "|  ORD012|2023-05-22|  Home & Kitchen|       0|          50.0|       urgent,gift|  ['Book1', 'Book2']|     Bob Smith | 23 Street Name, ...|\n",
      "|  ORD013|2022-12-30|unknown_category|       5|         43.86|              NULL|['Table', 'Chair'...| Charlie Brown | 769 Street Name,...|\n",
      "|  ORD014|2023-05-22|unknown_category|       3|          50.0|       urgent,gift|  ['Book1', 'Book2']|  David Wilson | 855 Street Name,...|\n",
      "|  ORD015|2022-12-30|           Books|       6|         89.92|['urgent', 'gift']|['Phone', 'Charge...|     Eva Davis | 672 Street Name,...|\n",
      "|  ORD016|2022-12-30|     Electronics|      10|          50.0|['urgent', 'gift']|                NULL|   Frank Miller|     unknown_address|\n",
      "|  ORD017|      NULL|     Electronics|       3|          50.0|    ['bulk_order']|['Laptop', 'Mouse...|     Grace Lee | 579 Street Name,...|\n",
      "|  ORD018|2023-05-22|unknown_category|      10|         70.59|       urgent,gift|['Phone', 'Charge...|   Henry Moore | 654 Street Name,...|\n",
      "|  ORD019|      NULL|unknown_category|       3|         72.02|       urgent,gift|['Table', 'Chair'...|    Ivy Taylor | 403 Street Name,...|\n",
      "|  ORD020|2022-12-30|  Home & Kitchen|       2|         19.55|    ['bulk_order']|                NULL|James Anderson | 210 Street Name,...|\n",
      "+--------+----------+----------------+--------+--------------+------------------+--------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now based on the \"customer_details\" column, we can split such information by \"|\" and create the following:\n",
    "#   - customer_name: split by | and get the first set of values (index 0)\n",
    "#   - customer_address: split by | and get the second set of values (index 1)\n",
    "#   - customer_address: replace NULL by \"unknown_address\"\n",
    "df = df.withColumn(\"customer_name\", split(col(\"customer_details\"), \"\\\\|\")[0])\n",
    "df = df.withColumn(\"customer_address\", split(col(\"customer_details\"), \"\\\\|\")[1])\n",
    "df = df.withColumn(\n",
    "    \"customer_address\", \n",
    "    when(col(\"customer_address\").isNull(), lit(\"unknown_address\")).otherwise(col(\"customer_address\"))\n",
    ")\n",
    "\n",
    "df = df.drop(\"customer_details\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|customer_address         |\n",
      "+-------------------------+\n",
      "| 584 Street Name, City 16|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check an example of customer_address, it contains Street Name and City, so we can create two more columns from it\n",
    "df.filter(col(\"order_id\") == \"ORD002\").select(\"customer_address\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+----+\n",
      "|customer_address         |street|city|\n",
      "+-------------------------+------+----+\n",
      "|unknown_address          |      |    |\n",
      "| 584 Street Name, City 16|584   |16  |\n",
      "| 598 Street Name, City 17|598   |17  |\n",
      "| 290 Street Name, City 12|290   |12  |\n",
      "| 387 Street Name, City 10|387   |10  |\n",
      "|unknown_address          |      |    |\n",
      "| 869 Street Name, City 16|869   |16  |\n",
      "| 661 Street Name, City 7 |661   |7   |\n",
      "| 239 Street Name, City 3 |239   |3   |\n",
      "| 760 Street Name, City 16|760   |16  |\n",
      "+-------------------------+------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Based on above cell, extract components from customer_address:\n",
    "#   - street_name: extract \"Street Name\"\n",
    "#   - city: extract \"City\"\n",
    "df = df.withColumn(\"street\", regexp_extract(col('customer_address'), r'(\\d+) Street Name', 1))\n",
    "df = df.withColumn(\"city\", regexp_extract(col('customer_address'), r'City (\\d+)', 1))\n",
    "df.select(\"customer_address\", \"street\", \"city\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------+--------+--------------+------------------+----------------------------+---------------+-------+-------+\n",
      "|order_id|order_date|product_category|quantity|price_per_unit|tags              |items                       |customer_name  |street |city   |\n",
      "+--------+----------+----------------+--------+--------------+------------------+----------------------------+---------------+-------+-------+\n",
      "|ORD001  |NULL      |Electronics     |4       |15.77         |['urgent', 'gift']|['Phone', 'Charger', 'Case']|Alice Johnson  |unknown|unknown|\n",
      "|ORD002  |2022-12-30|unknown_category|3       |50.0          |['bulk_order']    |['Book1', 'Book2']          |Bob Smith      |584    |16     |\n",
      "|ORD003  |2023-05-22|Books           |10      |79.64         |NULL              |['Book1', 'Book2']          |Charlie Brown  |598    |17     |\n",
      "|ORD004  |2022-12-30|Books           |0       |27.56         |NULL              |['Laptop', 'Mouse', None]   |David Wilson   |290    |12     |\n",
      "|ORD005  |NULL      |unknown_category|4       |50.0          |['bulk_order']    |['Phone', 'Charger', 'Case']|Eva Davis      |387    |10     |\n",
      "|ORD006  |2023-05-22|unknown_category|10      |50.0          |urgent,gift       |['Table', 'Chair', -1]      |Frank Miller   |unknown|unknown|\n",
      "|ORD007  |2023-05-22|Books           |0       |50.0          |['bulk_order']    |['Table', 'Chair', -1]      |Grace Lee      |869    |16     |\n",
      "|ORD008  |2022-12-30|unknown_category|3       |43.86         |urgent,gift       |['Table', 'Chair', -1]      |Henry Moore    |661    |7      |\n",
      "|ORD009  |NULL      |unknown_category|0       |50.0          |urgent,gift       |['Book1', 'Book2']          |Ivy Taylor     |239    |3      |\n",
      "|ORD010  |2023-01-15|Electronics     |4       |37.53         |urgent,gift       |NULL                        |James Anderson |760    |16     |\n",
      "+--------+----------+----------------+--------+--------------+------------------+----------------------------+---------------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add \"unknown\" for the empty string values on each column, and drop \"customer_address\" column\n",
    "df = df.withColumn(\n",
    "    \"street\", \n",
    "    when(col(\"street\") == \"\", lit(\"unknown\")).otherwise(col(\"street\"))\n",
    ") \\\n",
    ".withColumn(\n",
    "    \"city\", \n",
    "    when(col(\"city\") == \"\", lit(\"unknown\")).otherwise(col(\"city\"))\n",
    ")\n",
    "\n",
    "df = df.drop(\"customer_address\")\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|tags          |\n",
      "+--------------+\n",
      "|[urgent, gift]|\n",
      "|[bulk_order]  |\n",
      "|[]            |\n",
      "|[]            |\n",
      "|[bulk_order]  |\n",
      "|[urgent,gift] |\n",
      "|[bulk_order]  |\n",
      "|[urgent,gift] |\n",
      "|[urgent,gift] |\n",
      "|[urgent,gift] |\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_per_unit: double (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- items: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now lets fix \"items\" and \"tags\" columns, to set column type to Array, to do this first we need to convert the literals\n",
    "# \"['value']\" which is an string, into a real list, using the split function\n",
    "\n",
    "# Create idem-potent function to reuse for both columns that needs similar transformation\n",
    "def transform_col_to_array_type(df: SparkDataFrame, col_name: str) -> SparkDataFrame:\n",
    "    \"\"\"\n",
    "    This function will transform a column with StringType into ArrayType(StringType)\n",
    "    also replacing NULL for empty string, in an idem-potent way\n",
    "    \"\"\"\n",
    "    if not dict(df.dtypes)[col_name] == \"array<string>\":\n",
    "        df = df.withColumn(col_name, regexp_replace(col(col_name), r\"^\\[\", \"\")) \\\n",
    "            .withColumn(col_name, regexp_replace(col(col_name), r\"\\]$\", \"\")) \\\n",
    "            .withColumn(col_name, regexp_replace(col(col_name), r\"'\", \"\"))\n",
    "\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name).isNull(), \"\").otherwise(col(col_name))\n",
    "        )\n",
    "\n",
    "        df = df.withColumn(col_name, split(col(col_name), \", \"))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = transform_col_to_array_type(df=df, col_name=\"tags\")\n",
    "df.select(\"tags\").show(10, False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|items                 |\n",
      "+----------------------+\n",
      "|[Phone, Charger, Case]|\n",
      "|[Book1, Book2]        |\n",
      "|[Book1, Book2]        |\n",
      "|[Laptop, Mouse, None] |\n",
      "|[Phone, Charger, Case]|\n",
      "|[Table, Chair, -1]    |\n",
      "|[Table, Chair, -1]    |\n",
      "|[Table, Chair, -1]    |\n",
      "|[Book1, Book2]        |\n",
      "|[]                    |\n",
      "+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_per_unit: double (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same transformation process for \"items\"\n",
    "df = transform_col_to_array_type(df=df, col_name=\"items\")\n",
    "df.select(\"items\").show(10, False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Laptop',\n",
       " 'Table',\n",
       " 'Chair',\n",
       " 'Book2',\n",
       " 'Book1',\n",
       " 'Charger',\n",
       " 'Case',\n",
       " 'Phone',\n",
       " 'Mouse']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we can see there is still work to be done on the \"items\" column as on each row, inside the arrays\n",
    "# there are values like \"-1\", \"None\" that are also corrupting the data, so lets try to identify\n",
    "# if besides those two there are other values that need to be removed\n",
    "\n",
    "# Get a unique set of values by getting the set() of everything that is not \"None\" AND \"-1\"\n",
    "# explode() and collect() used here\n",
    "result = list()\n",
    "for items in df.select(\"items\", explode(\"items\")).collect():\n",
    "    result.extend([item for item in items[0] if not item == \"None\" and not item == \"-1\"])\n",
    "\n",
    "list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|items                 |\n",
      "+----------------------+\n",
      "|[Phone, Charger, Case]|\n",
      "|[Book1, Book2]        |\n",
      "|[Book1, Book2]        |\n",
      "|[Laptop, Mouse]       |\n",
      "|[Phone, Charger, Case]|\n",
      "|[Table, Chair]        |\n",
      "|[Table, Chair]        |\n",
      "|[Table, Chair]        |\n",
      "|[Book1, Book2]        |\n",
      "|[]                    |\n",
      "|[]                    |\n",
      "|[Book1, Book2]        |\n",
      "|[Table, Chair]        |\n",
      "|[Book1, Book2]        |\n",
      "|[Phone, Charger, Case]|\n",
      "|[]                    |\n",
      "|[Laptop, Mouse]       |\n",
      "|[Phone, Charger, Case]|\n",
      "|[Table, Chair]        |\n",
      "|[]                    |\n",
      "+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So based on the result above, we are confident that only \"None\" and \"-1\" are the values\n",
    "# to be removed for each array when present. Using array_remove() for this\n",
    "df = df.withColumn(\"items\", array_remove(\"items\", \"None\")).withColumn(\"items\", array_remove(\"items\", \"-1\"))\n",
    "df.select(\"items\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------+--------+--------------+--------------+----------------------+---------------+-------+-------+\n",
      "|order_id|order_date|product_category|quantity|price_per_unit|tags          |items                 |customer_name  |street |city   |\n",
      "+--------+----------+----------------+--------+--------------+--------------+----------------------+---------------+-------+-------+\n",
      "|ORD001  |NULL      |Electronics     |4       |15.77         |[urgent, gift]|[Phone, Charger, Case]|Alice Johnson  |unknown|unknown|\n",
      "|ORD002  |2022-12-30|unknown_category|3       |50.0          |[bulk_order]  |[Book1, Book2]        |Bob Smith      |584    |16     |\n",
      "|ORD003  |2023-05-22|Books           |10      |79.64         |[]            |[Book1, Book2]        |Charlie Brown  |598    |17     |\n",
      "|ORD004  |2022-12-30|Books           |0       |27.56         |[]            |[Laptop, Mouse]       |David Wilson   |290    |12     |\n",
      "|ORD005  |NULL      |unknown_category|4       |50.0          |[bulk_order]  |[Phone, Charger, Case]|Eva Davis      |387    |10     |\n",
      "|ORD006  |2023-05-22|unknown_category|10      |50.0          |[urgent,gift] |[Table, Chair]        |Frank Miller   |unknown|unknown|\n",
      "|ORD007  |2023-05-22|Books           |0       |50.0          |[bulk_order]  |[Table, Chair]        |Grace Lee      |869    |16     |\n",
      "|ORD008  |2022-12-30|unknown_category|3       |43.86         |[urgent,gift] |[Table, Chair]        |Henry Moore    |661    |7      |\n",
      "|ORD009  |NULL      |unknown_category|0       |50.0          |[urgent,gift] |[Book1, Book2]        |Ivy Taylor     |239    |3      |\n",
      "|ORD010  |2023-01-15|Electronics     |4       |37.53         |[urgent,gift] |[]                    |James Anderson |760    |16     |\n",
      "+--------+----------+----------------+--------+--------------+--------------+----------------------+---------------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_per_unit: double (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets check how the DataSet is now afer the transformations\n",
    "df.show(10, truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|order_date|\n",
      "+----------+\n",
      "|1900-01-01|\n",
      "|2022-12-30|\n",
      "|2023-05-22|\n",
      "|2022-12-30|\n",
      "|1900-01-01|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_per_unit: double (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First lets fill NULLs for a default date of \"1900-01-01\", then set the column to_date()\n",
    "df = df.withColumn(\"order_date\", when(col(\"order_date\").isNull(), lit(\"1900-01-01\")).otherwise(col(\"order_date\")))\n",
    "df = df.withColumn(\"order_date\", to_date(\"order_date\", \"yyyy-MM-dd\"))\n",
    "df.select(\"order_date\").show(5, truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------+--------+--------------+--------------+----------------------+---------------+-------+-------+\n",
      "|order_id|order_date|product_category|quantity|price_per_unit|tags          |items                 |customer_name  |street |city   |\n",
      "+--------+----------+----------------+--------+--------------+--------------+----------------------+---------------+-------+-------+\n",
      "|ORD001  |1900-01-01|Electronics     |4       |15.77         |[urgent, gift]|[Phone, Charger, Case]|Alice Johnson  |unknown|unknown|\n",
      "|ORD002  |2022-12-30|unknown_category|3       |50.0          |[bulk_order]  |[Book1, Book2]        |Bob Smith      |584    |16     |\n",
      "|ORD003  |2023-05-22|Books           |10      |79.64         |[]            |[Book1, Book2]        |Charlie Brown  |598    |17     |\n",
      "|ORD004  |2022-12-30|Books           |0       |27.56         |[]            |[Laptop, Mouse]       |David Wilson   |290    |12     |\n",
      "|ORD005  |1900-01-01|unknown_category|4       |50.0          |[bulk_order]  |[Phone, Charger, Case]|Eva Davis      |387    |10     |\n",
      "|ORD006  |2023-05-22|unknown_category|10      |50.0          |[urgent,gift] |[Table, Chair]        |Frank Miller   |unknown|unknown|\n",
      "|ORD007  |2023-05-22|Books           |0       |50.0          |[bulk_order]  |[Table, Chair]        |Grace Lee      |869    |16     |\n",
      "|ORD008  |2022-12-30|unknown_category|3       |43.86         |[urgent,gift] |[Table, Chair]        |Henry Moore    |661    |7      |\n",
      "|ORD009  |1900-01-01|unknown_category|0       |50.0          |[urgent,gift] |[Book1, Book2]        |Ivy Taylor     |239    |3      |\n",
      "|ORD010  |2023-01-15|Electronics     |4       |37.53         |[urgent,gift] |[]                    |James Anderson |760    |16     |\n",
      "|ORD011  |1900-01-01|unknown_category|4       |26.16         |[urgent,gift] |[]                    |Alice Johnson  |unknown|unknown|\n",
      "|ORD012  |2023-05-22|Home & Kitchen  |0       |50.0          |[urgent,gift] |[Book1, Book2]        |Bob Smith      |23     |17     |\n",
      "|ORD013  |2022-12-30|unknown_category|5       |43.86         |[]            |[Table, Chair]        |Charlie Brown  |769    |9      |\n",
      "|ORD014  |2023-05-22|unknown_category|3       |50.0          |[urgent,gift] |[Book1, Book2]        |David Wilson   |855    |16     |\n",
      "|ORD015  |2022-12-30|Books           |6       |89.92         |[urgent, gift]|[Phone, Charger, Case]|Eva Davis      |672    |2      |\n",
      "|ORD016  |2022-12-30|Electronics     |10      |50.0          |[urgent, gift]|[]                    |Frank Miller   |unknown|unknown|\n",
      "|ORD017  |1900-01-01|Electronics     |3       |50.0          |[bulk_order]  |[Laptop, Mouse]       |Grace Lee      |579    |14     |\n",
      "|ORD018  |2023-05-22|unknown_category|10      |70.59         |[urgent,gift] |[Phone, Charger, Case]|Henry Moore    |654    |12     |\n",
      "|ORD019  |1900-01-01|unknown_category|3       |72.02         |[urgent,gift] |[Table, Chair]        |Ivy Taylor     |403    |8      |\n",
      "|ORD020  |2022-12-30|Home & Kitchen  |2       |19.55         |[bulk_order]  |[]                    |James Anderson |210    |13     |\n",
      "+--------+----------+----------------+--------+--------------+--------------+----------------------+---------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# End result\n",
    "df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- When initially loading sample data (CSV, JSON) to define its quality, using an Schema based on StringType is a safe approach\n",
    "- Then by checking each column to infer which is the correct type to use, check which minimal transformation needs to be done\n",
    "- Casting directly a column to the desired type might lead to loose data that could be trasformed prior infering the Type\n",
    "- In some cases the context of the data in a column will allow to replace missing/corrupted data, like None, Null, -1, etc\n",
    "- Dig into PySpark SQL Functions in order to interact with the data in a more performant and safer way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-pyspark-IpMtr_Fy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
